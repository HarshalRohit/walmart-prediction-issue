{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('cnn_rnn': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8c9873bcc9ebb1b53c79c9b651d8a27c737712e5277a606925e1f19078210637"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Code\n",
    "# Change the proj_dir to deepspeech-2-ctc\n",
    "proj_home = '/home/rharshal/myProjects/thesis/deepspeech-2-ctc' \n",
    "proj_dir = '/home/rharshal/myProjects/thesis/deepspeech-2-ctc/src/VoyceWorks_Walmart'\n",
    "path_cnnrnn = f'{proj_dir}/CNN_RNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(path_cnnrnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_model import finalModel\n",
    "from data_generator import AudioGenerator\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import (BatchNormalization, Conv1D, Dense, Input, \n",
    "    TimeDistributed, Activation, Bidirectional, SimpleRNN, GRU, LSTM)\n",
    "import numpy as np\n",
    "from utils import int_sequence_to_text\n",
    "import nltk,pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termsPath = f'{path_cnnrnn}/termsWalmart.pickle'\n",
    "wordsPath = f'{path_cnnrnn}/wordsWalmart.pickle'\n",
    "#trainJsonPath='./CNN_RNN/json/nisheeth_wal_train.json'\n",
    "modelPath = f'{path_cnnrnn}/model/walmart_hindi.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(termsPath, 'rb') as handle:\n",
    "    terms=pickle.load(handle)\n",
    "    # print(\"Available terms \\n\",terms)\n",
    "with open(wordsPath, 'rb') as handle:\n",
    "    words=pickle.load(handle)\n",
    "    # print(\"Available words \\n\",words)\n",
    "with open(f'{path_cnnrnn}/mean_std/wal_hindi_train_data_std.pickle', 'rb') as handle:\n",
    "    std=pickle.load(handle)\n",
    "with open(f'{path_cnnrnn}/mean_std/wal_hindi_train_data_mean.pickle', 'rb') as handle:\n",
    "    mean=pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def after_edit_terms_sorting(after_edit_pred,terms):\n",
    "    terms_match_count=[]\n",
    "    after_edit_pred=set(after_edit_pred.split())\n",
    "    for term in terms :\n",
    "        terms_match_count.append((term,len(after_edit_pred.intersection(set(term.split())))))\n",
    "    terms_match_count = sorted(terms_match_count,key=lambda x :(x[1],len(x[0])),reverse=True)\n",
    "    return [x[0] for x in terms_match_count][:5]\n",
    "\n",
    "def match_strings(trans,pred):\n",
    "    tras_l=trans.split()\n",
    "    pred_l=pred.split()\n",
    "    c=0\n",
    "    for t in tras_l:\n",
    "        for p in pred_l:\n",
    "            if(t==p):\n",
    "                c=c+1\n",
    "    return len(tras_l),c\n",
    "\n",
    "def cnn_output_length(input_length, filter_size, border_mode, stride,\n",
    "                       dilation=1,conv_layers=1):\n",
    "    \"\"\" Compute the length of the output sequence after 1D convolution along\n",
    "        time. Note that this function is in line with the function used in\n",
    "        Convolution1D class from Keras.\n",
    "    Params:\n",
    "        input_length (int): Length of the input sequence.\n",
    "        filter_size (int): Width of the convolution kernel.\n",
    "        border_mode (str): Only support `same` or `valid`.\n",
    "        stride (int): Stride size used in 1D convolution.\n",
    "        dilation (int)\n",
    "    \"\"\"\n",
    "    if input_length is None:\n",
    "        return None\n",
    "    assert border_mode in {'same', 'valid'}\n",
    "    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
    "    if border_mode == 'same':\n",
    "        for layer in range(conv_layers):\n",
    "            output_length = (input_length + stride - 1) // stride\n",
    "            input_length=output_length\n",
    "    elif border_mode == 'valid':\n",
    "        for layer in range(conv_layers):\n",
    "            output_length = input_length - dilated_filter_size + 1\n",
    "            output_length=(output_length + stride - 1) // stride\n",
    "            input_length=output_length\n",
    "            \n",
    "    return output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model(input_dim, filters, kernel_size, conv_stride,\n",
    "    conv_border_mode, units, output_dim=29, dropout_rate=0.5, number_of_layers=2, \n",
    "    cell=GRU, activation='tanh',conv_layers=1):\n",
    "    \"\"\" Build a deep network for speech \n",
    "    \"\"\"\n",
    "    # Main acoustic input\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    # TODO: Specify the layers in your network\n",
    "    conv_1d = Conv1D(filters, kernel_size, \n",
    "                     strides=conv_stride, \n",
    "                     padding=conv_border_mode,\n",
    "                     activation='relu',\n",
    "                     name='layer_1_conv',\n",
    "                     dilation_rate=1)(input_data)\n",
    "    conv_bn = BatchNormalization(name='conv_batch_norm')(conv_1d)\n",
    "    conv_1d_2 = Conv1D(filters, kernel_size, \n",
    "                     strides=conv_stride, \n",
    "                     padding=conv_border_mode,\n",
    "                     activation='relu',\n",
    "                     name='layer_2_conv',\n",
    "                     dilation_rate=1)(conv_bn)\n",
    "    conv_bn_2 = BatchNormalization(name='conv_batch_norm_2')(conv_1d_2)\n",
    "\n",
    "\n",
    "    if number_of_layers == 1:\n",
    "        layer = cell(units, activation=activation,\n",
    "            return_sequences=True, implementation=2, name='rnn_1', dropout=dropout_rate, reset_after=False)(conv_bn_2)\n",
    "        layer = BatchNormalization(name='bt_rnn_1')(layer)\n",
    "    else:\n",
    "        layer = cell(units, activation=activation,\n",
    "                    return_sequences=True, implementation=2, name='rnn_1', dropout=dropout_rate, reset_after=False)(conv_bn_2)\n",
    "        layer = BatchNormalization(name='bt_rnn_1')(layer)\n",
    "\n",
    "        for i in range(number_of_layers - 2):\n",
    "            layer = cell(units, activation=activation,\n",
    "                        return_sequences=True, implementation=2, name='rnn_{}'.format(i+2), dropout=dropout_rate, reset_after=False)(layer)\n",
    "            layer = BatchNormalization(name='bt_rnn_{}'.format(i+2))(layer)\n",
    "\n",
    "        layer = cell(units, activation=activation,\n",
    "                    return_sequences=True, implementation=2, name='final_layer_of_rnn', reset_after=False)(layer)\n",
    "        layer = BatchNormalization(name='bt_rnn_final')(layer)\n",
    "    \n",
    "\n",
    "    time_dense = TimeDistributed(Dense(output_dim))(layer)\n",
    "    # TODO: Add softmax activation layer\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "    # Specify the model\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    # TODO: Specify model.output_length\n",
    "    model.output_length = lambda x: cnn_output_length(\n",
    "        x, kernel_size, conv_border_mode, conv_stride,conv_layers=conv_layers)\n",
    "#     model.output_length = 33\n",
    "    # print(model.summary())\n",
    "    return model\n",
    "\n",
    "def normalize_test(feature,mean,std,eps=1e-14):\n",
    "    return (feature - mean) / (std + eps)\n",
    "\n",
    "# specify the model\n",
    "# specify the model\n",
    "model_end = final_model(input_dim=161,\n",
    "                        filters=200,\n",
    "                        kernel_size=11, \n",
    "                        conv_stride=2,\n",
    "                        conv_border_mode='valid',\n",
    "                        units=250,\n",
    "                        activation='relu',\n",
    "                        cell=GRU,\n",
    "                        dropout_rate=0.9,\n",
    "                        number_of_layers=2,conv_layers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = AudioGenerator(spectrogram=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_end.load_weights(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Code\n",
    "# os.listdir(f'{proj_dir}/data/someWavFiles')\n",
    "# audio_path=sys.argv[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My code\n",
    "audio_filename = 'milgaya_1574415169408'\n",
    "audio_path = f'{proj_dir}/data/{audio_filename}.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data_gen.load_train_data( desc_file=trainJsonPath)\n",
    "# audio_path='uploads_Walmart_Lab/574414856883.wav'\n",
    "data_point = normalize_test(data_gen.featurize(audio_path),mean,std)\n",
    "data_point.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_end.predict(np.expand_dims(data_point, axis=0))\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_length = [model_end.output_length(data_point.shape[0])] \n",
    "output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = K.ctc_decode(pred, output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ints = (K.eval(K.ctc_decode(pred, output_length)[0][0])+1).flatten().tolist()\n",
    "pred_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_l=int_sequence_to_text(pred_ints)\n",
    "# print(pred_l)\n",
    "pred_text=''.join(pred_l)\n",
    "pred_text"
   ]
  },
  {
   "source": [
    "## Try Using awni's ctc-decoder found from [here](https://distill.pub/2017/ctc/#inference)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_awni_ctc = f'{proj_home}/try-ctc-decode-awni'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(dir_awni_ctc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.ctc_decoder as awni_ctc_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awni_ctc_decoder.decode(pred[0], blank=28)"
   ]
  },
  {
   "source": [
    "## My attempt to demystify the working of ctc_decode\n",
    "+ Tried to select the max prob for each time-step\n",
    "+ Then mapped to chars based on mapping defined in *char_map.py*\n",
    "\n",
    "Seems to work fine, but after reading [this](https://distill.pub/2017/ctc/) my observations have changed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "throw ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pred[0]\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr_map = { idx:char for (idx, char) in enumerate('` abcdefghijklmnopqrstuvwxyz$') }\n",
    "print(chr_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_argmax = np.argmax(tmp, axis=1)\n",
    "print(tmp_argmax.shape)\n",
    "tmp_argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_pred = [ chr_map[idx] for idx in tmp_argmax ]\n",
    "''.join(tmp_pred)"
   ]
  }
 ]
}